{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30579,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization Project"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison of SGD an SFW with different Learning Rates"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install ucimlrepo\n",
    "!pip install --upgrade certifi"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import itertools\n",
    "import ssl\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Tensorflow version: \", tf.__version__)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"List GPU devices: \", tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Simple DNN with 2 Hidden Layer on A Multivariate Dataset with 7 Classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dry Bean Dataset Description\n",
    "\n",
    "## Dataset Overview\n",
    "- **Number of Instances:** 13,611\n",
    "- **Number of Features:** 16\n",
    "- **Feature Types:** Integer, Real\n",
    "- **Subject Area:** Biology\n",
    "- **Associated Tasks:** Classification\n",
    "- **Data Type:** Multivariate\n",
    "\n",
    "## Dataset Information\n",
    "This dataset consists of images capturing 13,611 grains of seven different registered dry beans, taken with a high-resolution camera. The primary purpose of the dataset is to support classification tasks, specifically to distinguish between seven varieties of dry beans based on various features related to form, shape, type, and structure.\n",
    "\n",
    "## Features\n",
    "1. **Area (A):** The area of a bean zone and the number of pixels within its boundaries.\n",
    "2. **Perimeter (P):** Bean circumference, defined as the length of its border.\n",
    "3. **Major Axis Length (L):** Distance between the ends of the longest line that can be drawn from a bean.\n",
    "4. **Minor Axis Length (l):** The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
    "5. **Aspect Ratio (K):** Defines the relationship between L and l.\n",
    "6. **Eccentricity (Ec):** Eccentricity of the ellipse having the same moments as the region.\n",
    "7. **Convex Area (C):** Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
    "8. **Equivalent Diameter (Ed):** The diameter of a circle having the same area as a bean seed area.\n",
    "9. **Extent (Ex):** The ratio of the pixels in the bounding box to the bean area.\n",
    "10. **Solidity (S):** Also known as convexity, the ratio of the pixels in the convex shell to those found in beans.\n",
    "11. **Roundness (R):** Calculated with the formula: (4πA)/(P^2).\n",
    "12. **Compactness (CO):** Measures the roundness of an object: Ed/L.\n",
    "13. **ShapeFactor1 (SF1):** Feature.\n",
    "14. **ShapeFactor2 (SF2):** Feature.\n",
    "15. **ShapeFactor3 (SF3):** Feature.\n",
    "16. **ShapeFactor4 (SF4):** Feature.\n",
    "\n",
    "## Target Variable\n",
    "- **Class:** Categorical variable indicating the type of dry bean. Possible classes are Seker, Barbunya, Bombay, Cali, Dermosan, Horoz, and Sira.\n",
    "\n",
    "## Units and Missing Values\n",
    "- Units are specified for relevant features.\n",
    "- No missing values are reported in the provided information.\n",
    "\n",
    "## Dataset Creation and Purpose\n",
    "This dataset was curated to develop a classification model capable of distinguishing between different varieties of dry beans based on high-resolution images and extracted features. The features include both geometrical dimensions and shape forms, providing a comprehensive set for robust classification.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# fetch dataset \n",
    "dry_bean_dataset = fetch_ucirepo(id=602)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = dry_bean_dataset.data.features\n",
    "y = dry_bean_dataset.data.targets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:19.928428256Z",
     "start_time": "2023-10-18T14:40:19.917251778Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_names = ['Seker', 'Barbunya', 'Bombay', 'Cali', 'Dermosan', 'Horoz', 'Sira']"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = X.values\n",
    "y = y.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:22.346137396Z",
     "start_time": "2023-10-18T14:40:22.332784508Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def normalize_features(input_data):\n",
    "    return StandardScaler().fit_transform(input_data)\n",
    "\n",
    "\n",
    "def one_hot_label_encoder(input_data):\n",
    "    return OneHotEncoder().fit_transform(input_data.reshape(-1, 1)).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:23.018664570Z",
     "start_time": "2023-10-18T14:40:23.001979685Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X = normalize_features(X)\n",
    "y = one_hot_label_encoder(y)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:23.582037252Z",
     "start_time": "2023-10-18T14:40:23.567902303Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_loss_curve(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper right')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_accuracy_curve(history):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_precision_curve(history):\n",
    "    plt.plot(history.history['precision'])\n",
    "    plt.plot(history.history['val_precision'])\n",
    "    plt.title('Model Precision')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_recall_curve(history):\n",
    "    plt.plot(history.history['recall'])\n",
    "    plt.plot(history.history['val_recall'])\n",
    "    plt.title('Model Recall')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='lower right')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(dnn_model, features, targets, classes, dataset):\n",
    "    y_test_preds = dnn_model.predict(features, verbose='auto', steps=test_steps)\n",
    "    y_test_preds = np.argmax(y_test_preds, axis=1)\n",
    "    y_test = np.argmax(targets, axis=1)\n",
    "    if dataset == 'cifar10':\n",
    "        cm = confusion_matrix(y_test, y_test_preds, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    elif dataset == 'dry_bean':\n",
    "        cm = confusion_matrix(y_test, y_test_preds, labels=[0, 1, 2, 3, 4, 5, 6])\n",
    "    else:\n",
    "        raise ValueError('dataset should be one of cifar10 or dry_bean.')\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Oranges)\n",
    "    plt.title('Confusion Matrix')\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.colorbar()\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:24.476072365Z",
     "start_time": "2023-10-18T14:40:24.469302778Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_steps = np.ceil(X_train.shape[0] / batch_size)\n",
    "test_steps = np.ceil(X_test.shape[0] / batch_size)\n",
    "val_steps = np.ceil(X_val.shape[0] / batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:25.516591828Z",
     "start_time": "2023-10-18T14:40:25.495847016Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_f1_score(p, r):\n",
    "    precision = p.result().numpy()\n",
    "    recall = r.result().numpy()\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dnn_model(input_shape, num_of_hidden, units, activations, num_of_classes):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(input_shape,)))\n",
    "    for i in range(num_of_hidden):\n",
    "        model.add(tf.keras.layers.Dense(units[i], activations[i]))\n",
    "    if num_of_classes == 1:\n",
    "        model.add(tf.keras.layers.Dense(num_of_classes, activation='linear'))\n",
    "        return model\n",
    "    elif num_of_classes == 2:\n",
    "        model.add(tf.keras.layers.Dense(num_of_classes, activation='sigmoid'))\n",
    "        return model\n",
    "    model.add(tf.keras.layers.Dense(num_of_classes, activation='softmax'))\n",
    "    return model"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def reset_model_parameters(model):\n",
    "    for layer in model.layers:\n",
    "        layer.set_weights([\n",
    "            tf.keras.initializers.GlorotUniform()(shape=layer.get_weights()[0].shape),\n",
    "            tf.zeros_like(layer.get_weights()[1])\n",
    "        ])\n",
    "    print('All model parameters have been reinitilized.')\n",
    "    return"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "precision_metric = tf.keras.metrics.Precision()\n",
    "recall_metric = tf.keras.metrics.Recall()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# learning_rates = []"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PrintLearningRateCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        learning_rate = self.model.optimizer.lr.numpy()\n",
    "        print(f\"\\nLearning Rate at the end of epoch {epoch + 1}: {learning_rate}\\n\")\n",
    "\n",
    "#         learning_rates.append(learning_rate)\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_recall',\n",
    "    mode='max',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SGD as optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "model = create_dnn_model(input_shape=X_train.shape[1], num_of_hidden=2, units=[16, 8], activations=['relu', 'relu'],\n",
    "                         num_of_classes=len(class_names))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:31.271721606Z",
     "start_time": "2023-10-18T14:40:31.264316119Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:37.613020814Z",
     "start_time": "2023-10-18T14:40:32.004262868Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with SGD optimizer:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T14:40:40.306407508Z",
     "start_time": "2023-10-18T14:40:40.049705349Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stochastic Frank-Wolfe with L1 ball as feasible region as optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class MyLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, method, params):\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.method = method\n",
    "        self.params = params\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.method == \"FIX\":\n",
    "            return self.initial_learning_rate\n",
    "\n",
    "        elif self.method == \"STEP\":\n",
    "            gamma = self.params.get(\"gamma\")\n",
    "            l = self.params.get(\"l\")\n",
    "            return self.initial_learning_rate * tf.pow(gamma, tf.math.floor(tf.cast(step, tf.float32) / l))\n",
    "\n",
    "        elif self.method == \"EXP\":\n",
    "            gamma = self.params.get(\"gamma\")\n",
    "            return self.initial_learning_rate * tf.pow(gamma, tf.cast(step, tf.float32))\n",
    "\n",
    "        elif self.method == \"INV\":\n",
    "            gamma = self.params.get(\"gamma\")\n",
    "            p = self.params.get(\"p\")\n",
    "            return self.initial_learning_rate / tf.pow((1 + (gamma * tf.cast(step, tf.float32))), p)\n",
    "\n",
    "        elif self.method == \"POLY\":\n",
    "            l = self.params.get(\"l\")\n",
    "            p = self.params.get(\"p\")\n",
    "            return self.initial_learning_rate * tf.pow((1 - (tf.cast(step, tf.float32) / l)), p)\n",
    "\n",
    "        elif self.method in [\"TRI\", \"TRI2\", \"TRIEXP\", \"SIN\", \"SIN2\", \"SINEXP\", \"COS\"]:\n",
    "            l = self.params.get(\"l\")\n",
    "            k0 = self.initial_learning_rate\n",
    "            k1 = self.params.get(\"k1\")\n",
    "            gamma = self.params.get(\"gamma\")\n",
    "            if self.method == \"TRI\":\n",
    "                return k0 + (k1 - k0) * tf.abs(\n",
    "                    tf.math.asin(tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))) / (2 / math.pi)\n",
    "\n",
    "            elif self.method == \"TRI2\":\n",
    "                return k0 + (k1 - k0) * (1 / (tf.pow(2, tf.math.floor(tf.cast(step, tf.float32) / (2 * l))))) * tf.abs(\n",
    "                    tf.math.asin(tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))) / (2 / math.pi)\n",
    "\n",
    "            elif self.method == \"TRIEXP\":\n",
    "                return (k0 + (k1 - k0) * tf.abs(\n",
    "                    tf.math.asin(tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))) / (\n",
    "                                math.pi / 2)) * tf.pow(gamma, tf.cast(step, tf.float32))\n",
    "\n",
    "            elif self.method == \"SIN\":\n",
    "                return k0 + (k1 - k0) * tf.abs(tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))\n",
    "\n",
    "            elif self.method == \"SIN2\":\n",
    "                return k0 + (k1 - k0) * (1 / (tf.pow(2, tf.math.floor(tf.cast(step, tf.float32) / (2 * l))))) * tf.abs(\n",
    "                    tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))\n",
    "\n",
    "            elif self.method == \"SINEXP\":\n",
    "                return (k0 + (k1 - k0) * tf.abs(\n",
    "                    tf.math.sin(math.pi * tf.cast(step, tf.float32) / (2 * l)))) * tf.pow(gamma,\n",
    "                                                                                          tf.cast(step, tf.float32))\n",
    "\n",
    "            elif self.method == \"COS\":\n",
    "                return k0 + (k1 - k0) * 0.5 * (1 + tf.math.cos(math.pi * 2 * tf.cast(step, tf.float32) / l))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"The given method does not exist.\")\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'initial_learning_rate': self.initial_learning_rate,\n",
    "            'method': self.method,\n",
    "            'params': self.method\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class StochasticFrankWolfe(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=False,\n",
    "            weight_decay=None,\n",
    "            clipnorm=None,\n",
    "            clipvalue=None,\n",
    "            global_clipnorm=None,\n",
    "            use_ema=False,\n",
    "            ema_momentum=0.99,\n",
    "            ema_overwrite_frequency=None,\n",
    "            jit_compile=True,\n",
    "            name=\"StochasticFrankWolfe\",\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            weight_decay=weight_decay,\n",
    "            clipnorm=clipnorm,\n",
    "            clipvalue=clipvalue,\n",
    "            global_clipnorm=global_clipnorm,\n",
    "            use_ema=use_ema,\n",
    "            ema_momentum=ema_momentum,\n",
    "            ema_overwrite_frequency=ema_overwrite_frequency,\n",
    "            jit_compile=jit_compile,\n",
    "            **kwargs\n",
    "        )\n",
    "        self._learning_rate = self._build_learning_rate(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        if isinstance(momentum, (int, float)) and (momentum < 0 or momentum > 1):\n",
    "            raise ValueError(\"`momentum` must be between [0, 1].\")\n",
    "\n",
    "    def build(self, var_list):\n",
    "        super().build(var_list)\n",
    "        if hasattr(self, \"_built\") and self._built:\n",
    "            return\n",
    "        self._built = True\n",
    "        self.momentums = []\n",
    "        for var in var_list:\n",
    "            self.momentums.append(\n",
    "                self.add_variable_from_reference(\n",
    "                    model_variable=var, variable_name=\"m\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def update_step(self, gradient, variable):\n",
    "        # Cast the learning rate and the iteration count to the dtype of the variable\n",
    "        lr = tf.cast(self.learning_rate, variable.dtype)  # Corresponds to αt in Algorithm 1\n",
    "        local_step = tf.cast(self.iterations + 1, variable.dtype)  # Corresponds to t in Algorithm 1\n",
    "\n",
    "        # Compute the step size as learning rate divided by the iteration count\n",
    "        step_size = lr / local_step  # This is a common way to decay the learning rate in stochastic optimization algorithms\n",
    "\n",
    "        # Cast the momentum to the dtype of the variable\n",
    "        momentum = tf.cast(self.momentum, variable.dtype)  # Corresponds to ρt in Algorithm 1\n",
    "\n",
    "        # Get the momentum variable for this variable\n",
    "        m = self.momentums[self._index_dict[self._var_key(variable)]]  # Corresponds to mt in Algorithm 1\n",
    "\n",
    "        # Scale the gradient by its L1 norm\n",
    "        scaled_gradient = gradient / tf.norm(gradient,\n",
    "                                             ord=1)  # This is a specific choice made in your implementation, not directly specified in Algorithm 1\n",
    "\n",
    "        # Update the momentum variable with the current gradient and the previous momentum\n",
    "        m.assign((1. - momentum) * scaled_gradient + momentum * m)  # Corresponds to Line 5 in Algorithm 1\n",
    "\n",
    "        # Compute the direction of the update as the sign of the negative momentum\n",
    "        v = tf.sign(-m)  # Corresponds to Line 6 in Algorithm 1, assuming the feasible region C is the L1 ball\n",
    "\n",
    "        # Perform the update\n",
    "        if isinstance(gradient, tf.IndexedSlices):\n",
    "            # If the gradient is sparse, perform a sparse update\n",
    "            indices = gradient.indices\n",
    "            updates = step_size * (v - variable)\n",
    "            variable.scatter_nd_update(indices[:, None],\n",
    "                                       updates)  # This is a TensorFlow-specific way to handle sparse updates\n",
    "        else:\n",
    "            # If the gradient is dense, perform a dense update\n",
    "            variable.assign_add(step_size * (v - variable))  # Corresponds to Line 7 in Algorithm 1\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\n",
    "                    self._learning_rate\n",
    "                ),\n",
    "                \"momentum\": self.momentum,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#FIX\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(\n",
    "        learning_rate=MyLearningRateSchedule(initial_learning_rate=0.1, method='FIX', params={}), momentum=0.9),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and FIX Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#EXP\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(\n",
    "        learning_rate=MyLearningRateSchedule(initial_learning_rate=0.5, method='EXP', params={'gamma': 0.999}),\n",
    "        momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=50, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and EXP Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#INV\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=MyLearningRateSchedule(initial_learning_rate=0.5, method='INV',\n",
    "                                                                        params={'gamma': 0.001, 'p': 0.9}),\n",
    "                                   momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=40, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and INV Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#STEP\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=MyLearningRateSchedule(initial_learning_rate=0.5, method='STEP',\n",
    "                                                                        params={'gamma': 0.99, 'l': 130}),\n",
    "                                   momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and STEP Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#POLY\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=MyLearningRateSchedule(initial_learning_rate=0.1, method='POLY',\n",
    "                                                                        params={'p': 2, 'l': train_steps ** 2}),\n",
    "                                   momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and POLY Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#SINEXP\n",
    "model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=MyLearningRateSchedule(initial_learning_rate=0.05, method='SINEXP',\n",
    "                                                                        params={'k1': 0.05, 'l': 500, 'gamma': 0.999}),\n",
    "                                   momentum=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size=batch_size, epochs=100, shuffle=True,\n",
    "                          validation_data=(X_val, y_val),\n",
    "                          steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                          callbacks=[PrintLearningRateCallback()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(model_history)\n",
    "plot_accuracy_curve(model_history)\n",
    "plot_precision_curve(model_history)\n",
    "plot_recall_curve(model_history)\n",
    "\n",
    "plot_confusion_matrix(model, X_test, y_test, classes=class_names, dataset='dry_bean')\n",
    "\n",
    "model_score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DNN model classifier evaluation results with StochasticFrankWolfe optimizer and SINEXP Learning Rate:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "reset_model_parameters(model)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DenseNet121 on CIFAR-10 Dataset with SGD and SFW"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CIFAR-10 Dataset Description\n",
    "\n",
    "## Dataset Overview\n",
    "- **Number of Instances:** 60,000 (50,000 for training, 10,000 for testing)\n",
    "- **Number of Classes:** 10\n",
    "- **Data Type:** Multivariate\n",
    "- **Subject Area:** Computer Vision\n",
    "- **Associated Tasks:** Image Classification\n",
    "- **Image Dimensions:** 32x32 pixels with 3 color channels (RGB)\n",
    "\n",
    "## Dataset Information\n",
    "The CIFAR-10 dataset is a collection of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is split into a training set of 50,000 images and a test set of 10,000 images. Each class represents a distinct object or animal category.\n",
    "\n",
    "## Classes\n",
    "1. **Airplane**\n",
    "2. **Automobile**\n",
    "3. **Bird**\n",
    "4. **Cat**\n",
    "5. **Deer**\n",
    "6. **Dog**\n",
    "7. **Frog**\n",
    "8. **Horse**\n",
    "9. **Ship**\n",
    "10. **Truck**\n",
    "\n",
    "## Image Features\n",
    "- Each image is 32x32 pixels, and it has three color channels (RGB).\n",
    "- Total Features: \\(32 \\times 32 \\times 3 = 3072\\) features per image.\n",
    "\n",
    "## Target Variable\n",
    "- **Class Label:** Categorical variable indicating the class of the object or animal in the image.\n",
    "\n",
    "## Dataset Purpose\n",
    "CIFAR-10 is widely used in the field of computer vision for benchmarking image classification algorithms. The relatively small size of the images and the variety of classes make it a suitable dataset for testing and comparing the performance of different models.\n",
    "\n",
    "## Units and Missing Values\n",
    "- Pixel values in the RGB channels represent color intensity (0 to 255).\n",
    "- No missing values are reported as images are complete and standardized.\n",
    "\n",
    "## Additional Notes\n",
    "- CIFAR-10 serves as a standard benchmark in machine learning research and is often used for educational purposes due to its manageable size.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "split = 40000\n",
    "X_val, y_val = X_train[split:], y_train[split:]\n",
    "X_train, y_train = X_train[:split], y_train[:split]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_steps = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "test_steps = int(np.ceil(X_test.shape[0] / batch_size))\n",
    "val_steps = int(np.ceil(X_val.shape[0] / batch_size))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = tf.keras.applications.densenet.preprocess_input(X_train)\n",
    "X_val = tf.keras.applications.densenet.preprocess_input(X_val)\n",
    "X_test = tf.keras.applications.densenet.preprocess_input(X_test)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_train = one_hot_label_encoder(y_train)\n",
    "y_val = one_hot_label_encoder(y_val)\n",
    "y_test = one_hot_label_encoder(y_test)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.summary()"
   ],
   "metadata": {
    "_kg_hide-output": false,
    "scrolled": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.0005),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=45, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SGD optimizer:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#FIX\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(\n",
    "        learning_rate=MyLearningRateSchedule(initial_learning_rate=0.1, method='FIX', params={}), momentum=0.9),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=50, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#EXP\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.5, momentum=0.99, lr_schedule='EXP',\n",
    "                                   schedule_params={'gamma': 0.01}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, densenet_model.optimizer.lr.numpy()))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=50, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps,\n",
    "                                            callbacks=[es, PrintLR()], verbose='auto')"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#INV\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.001, momentum=0.99, lr_schedule='INV',\n",
    "                                   schedule_params={'gamma': 0.09}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#POLY\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.001, momentum=0.99, lr_schedule='POLY',\n",
    "                                   schedule_params={'l': train_steps, 'p': 1.1}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#TRI\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.001, momentum=0.99, lr_schedule='TRI',\n",
    "                                   schedule_params={'k0': 0.0001, 'k1': 0.001, 'l': 3, 'gamma': 0.9}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#SINEXP\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.001, momentum=0.99, lr_schedule='SINEXP',\n",
    "                                   schedule_params={'k0': 0.0001, 'k1': 0.001, 'l': 3, 'gamma': 0.9}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#COS\n",
    "densenet_model = tf.keras.applications.densenet.DenseNet121(include_top=True, weights=None,\n",
    "                                                            input_shape=X_train.shape[1:],\n",
    "                                                            pooling='avg', classes=len(class_names),\n",
    "                                                            classifier_activation='softmax')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model.compile(\n",
    "    optimizer=StochasticFrankWolfe(learning_rate=0.001, momentum=0.99, lr_schedule='COS',\n",
    "                                   schedule_params={'k0': 0.0001, 'k1': 0.001, 'l': 3, 'gamma': 0.9}),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy', precision_metric, recall_metric])"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "densenet_model_history = densenet_model.fit(X_train, y_train, batch_size=batch_size, epochs=20, shuffle=True,\n",
    "                                            validation_data=(X_val, y_val),\n",
    "                                            steps_per_epoch=train_steps, validation_steps=val_steps, callbacks=[es],\n",
    "                                            verbose='auto')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plot_loss_curve(densenet_model_history)\n",
    "plot_accuracy_curve(densenet_model_history)\n",
    "\n",
    "plot_confusion_matrix(densenet_model, X_test, y_test, classes=class_names, dataset='cifar10')\n",
    "\n",
    "densenet_model_score = densenet_model.evaluate(X_test, y_test, batch_size=batch_size, verbose='auto', steps=test_steps)\n",
    "print(\"DenseNet model classifier evaluation results with SFW optimizer with fixed step size:\\n\")\n",
    "print('Test set Loss = {:.5f}'.format(densenet_model_score[0]))\n",
    "print('Test set Accuracy = {:.2f}'.format(densenet_model_score[1]))\n",
    "print('Test set Precision = {:.2f}'.format(densenet_model_score[2]))\n",
    "print('Test set Recall = {:.2f}'.format(densenet_model_score[3]))\n",
    "print('Test set F1 Score = {:.2f}'.format(get_f1_score(precision_metric, recall_metric)))"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
